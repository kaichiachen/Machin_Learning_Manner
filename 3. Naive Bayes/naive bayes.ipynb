{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes (朴素贝叶斯)\n",
    "前面讲的knn以及decision tree都是属于分类模型，并且此分类模型为hard type，接下来我们讲的朴素贝叶斯也是分类模型，但是是soft type，也就是会计算出各个种类的概率。  \n",
    "朴素贝叶斯作为贝叶斯定理的分支，我们先从贝叶斯定理讲起。贝叶斯定理基于条件概率从已知判断未知，首先，介绍条件概率的公式：\n",
    "$$P(A|B) = \\frac{P(A)P(B|A)}{P(B)}$$\n",
    "if B 发生后A的概率如上所示，以现实情况就是，如果被诊断出癌症(B)，那么真的是癌症(A)的概率是多少？首先，你有一个得癌症概率的数据$p_a$，在医生诊断之前，得癌症的概率就是$p_a$，然而，如今被诊断出来，自己得癌症的概率也不代表就是100%，你还有误诊的概率数据$p_b$以及医生诊断病人为癌症的概率$p_c$，显而易见的，$p_a=p_c(1-p_b)+(1-p_c)p_b$，所以被诊断出癌症后，罹癌概率从$p_a$变成了$\\frac{p_c\\times (1-p_b)}{p_a}$\n",
    "\n",
    "朴素贝叶斯可以想成是贝叶斯定理里条件概率的B有多个特征，变成是\n",
    "$$P(A|B,C) = \\frac{P(A)P(B|A)P(C|A,B)}{P(B)P(C|B)}$$\n",
    "朴素贝叶斯里的朴素两个字意思是假设特征之间在统计的意义上是独立的，也就是相关系数为0，所以完整的朴素贝叶斯公式为如下形式：\n",
    "$$P(A|B,C) = \\frac{P(A)P(B|A)P(C|A)}{P(B)*P(C)}$$\n",
    "\n",
    "### 优缺点\n",
    "\n",
    "优点：数据少也能训练出不错的结果、可以同时处理多个classes  \n",
    "缺点：对数据的输入敏感\n",
    "\n",
    "### 示例代码\n",
    "我们以判断一句话是否为辱骂性(abusive)的句子为例  \n",
    "首先，loadDataSet代表产生训练数据，返回训练数据以及标签，createVocabList则是将训练数据取set，记录训练集出现的单词，setOfWords2Vec则是将原始训练数据(单词)转成只有0和1的向量形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0,1,0,1,0,1]    #1 is abusive, 0 not\n",
    "    return postingList,classVec\n",
    "                 \n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])  #create empty set\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document) #union of the two sets\n",
    "    return list(vocabSet)\n",
    "\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else: print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了句子的向量形式以及标签，我们利用trainNB0进行训练，其输出三个值，分别为各个单词代表非辱骂性的概率、各个单词代表辱骂性的概率、以及整体训练集是辱骂性句子的比例，计算概率的方式就是最直觉的方式 - 出现这个词的次数/这个类里单词的总数  \n",
    "我们以stupid这个单词为例，在辱骂性的句子里，stupid出现3次，辱骂性句子总共有19个单词，所以概率就是$\\frac{3}{19}$  \n",
    "然而我们知道朴素贝叶斯的公式假设特征之间独立，所以会连续相乘，如果遇到概率为0，不管之后多大相乘都还是0，例如stupid在非辱骂性的概率就是0，所以有个办法是得出概率后加上一个极小值,并且对概率取log方便计算，因为$log(ab)=log(a)+log(b)$,避免之后计算朴素贝叶斯公式时，数值连续相乘导致逼近于0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)\n",
    "    p0Num = np.ones(numWords); p1Num = np.ones(numWords)      #change to ones() \n",
    "    p0Denom = 2.0; p1Denom = 2.0                        #change to 2.0\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    p1Vect = np.log(p1Num/p1Denom)          #change to log()\n",
    "    p0Vect = np.log(p0Num/p0Denom)          #change to log()\n",
    "    return p0Vect,p1Vect,pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "postingList,classVec = loadDataSet()\n",
    "vob_list = createVocabList(postingList)\n",
    "trainMat=[]\n",
    "for sentence in postingList:\n",
    "    trainMat.append(setOfWords2Vec(vob_list, sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0V, p1V, pAb = trainNB0(trainMat, classVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.258096538021482 -2.3513752571634776 park\n",
      "-3.258096538021482 -2.3513752571634776 posting\n",
      "-2.5649493574615367 -2.3513752571634776 stop\n",
      "-2.5649493574615367 -3.044522437723423 cute\n",
      "-2.5649493574615367 -3.044522437723423 problems\n",
      "-3.258096538021482 -2.3513752571634776 quit\n",
      "-3.258096538021482 -2.3513752571634776 maybe\n",
      "-3.258096538021482 -2.3513752571634776 not\n",
      "-2.5649493574615367 -3.044522437723423 mr\n",
      "-2.5649493574615367 -2.3513752571634776 to\n",
      "-2.5649493574615367 -3.044522437723423 how\n",
      "-2.5649493574615367 -3.044522437723423 ate\n",
      "-2.5649493574615367 -3.044522437723423 is\n",
      "-3.258096538021482 -1.9459101490553135 worthless\n",
      "-2.5649493574615367 -1.9459101490553135 dog\n",
      "-2.5649493574615367 -3.044522437723423 so\n",
      "-1.8718021769015913 -3.044522437723423 my\n",
      "-2.5649493574615367 -3.044522437723423 help\n",
      "-3.258096538021482 -2.3513752571634776 buying\n",
      "-3.258096538021482 -2.3513752571634776 garbage\n",
      "-3.258096538021482 -2.3513752571634776 food\n",
      "-2.5649493574615367 -3.044522437723423 please\n",
      "-2.5649493574615367 -3.044522437723423 steak\n",
      "-3.258096538021482 -2.3513752571634776 take\n",
      "-2.5649493574615367 -3.044522437723423 licks\n",
      "-2.5649493574615367 -3.044522437723423 flea\n",
      "-3.258096538021482 -1.6582280766035324 stupid\n",
      "-2.5649493574615367 -3.044522437723423 has\n",
      "-2.159484249353372 -2.3513752571634776 him\n",
      "-2.5649493574615367 -3.044522437723423 love\n",
      "-2.5649493574615367 -3.044522437723423 dalmation\n",
      "-2.5649493574615367 -3.044522437723423 I\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(vob_list)):\n",
    "    print(p0V[i],p1V[i],vob_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来就是利用朴素贝叶斯计算各类别的概率了，我们再放一次朴素贝叶斯的公式\n",
    "$$P(A|B,C) = \\frac{P(A)P(B|A)P(C|A)}{P(B)*P(C)}$$\n",
    "A代表类别，也就是是否为辱骂性的句子，B、C、D...代表特征，这里的特征就是单词，由于我们是计算相对的值来判定类别，所以分母$P(B)*P(C)$可以不用计算，由于我们之前计算的概率取了log，所以分子相乘的运算就是log相加的运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vec2Classify * p1Vec) + np.log(pClass1)    #element-wise mult\n",
    "    p0 = sum(vec2Classify * p0Vec) + np.log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "def testingNB():\n",
    "    listOPosts,listClasses = loadDataSet()\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    trainMat=[]\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    p0V,p1V,pAb = trainNB0(np.array(trainMat),np.array(listClasses))\n",
    "    testEntry = ['love', 'my', 'dalmation']\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb))\n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as:  0\n",
      "['stupid', 'garbage'] classified as:  1\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上的例子，我们只对词做出现或缺失的操作，接下来介绍的是bag-of-words，将词语的频率也考虑进去，我们以分辨垃圾邮件为例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagOfWords2VecMN(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:4: FutureWarning: split() requires a non-empty pattern match.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "regEx=re.compile('\\\\W*')\n",
    "emailText = open('./datasets/email/ham/6.txt').read()\n",
    "listOfTokens=regEx.split(emailText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification error ['yay', 'you', 'both', 'doing', 'fine', 'working', 'mba', 'design', 'strategy', 'cca', 'top', 'art', 'school', 'new', 'program', 'focusing', 'more', 'right', 'brained', 'creative', 'and', 'strategic', 'approach', 'management', 'the', 'way', 'done', 'today']\n",
      "the error rate is:  0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def textParse(bigString):    #input is big string, #output is word list\n",
    "    import re\n",
    "    listOfTokens = re.split(r'\\W*', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2] \n",
    "    \n",
    "def spamTest():\n",
    "    docList=[]; classList = []; fullText =[]\n",
    "    for i in range(1,26):\n",
    "        wordList = textParse(open('./datasets/email/spam/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = textParse(open('./datasets/email/ham/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = createVocabList(docList)#create vocabulary\n",
    "    trainingSet = list(range(50)); testSet=[]           #create test set\n",
    "    for i in range(10):\n",
    "        randIndex = int(random.uniform(0,len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del trainingSet[randIndex]\n",
    "    trainMat=[]; trainClasses = []\n",
    "    for docIndex in trainingSet:#train the classifier (get probs) trainNB0\n",
    "        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V,p1V,pSpam = trainNB0(np.array(trainMat),np.array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:        #classify the remaining items\n",
    "        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])\n",
    "        if classifyNB(np.array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "            print(\"classification error\",docList[docIndex])\n",
    "    print('the error rate is: ',float(errorCount)/len(testSet))\n",
    "    #return vocabList,fullText\n",
    "spamTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
