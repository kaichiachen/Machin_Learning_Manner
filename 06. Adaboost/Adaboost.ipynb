{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost\n",
    "Adaboost为一种集成学习方法，集成学习利用多个模型以同个输入的预测结果进行投票，通常可以提升2~3%的精准度，集成学习大致上分为两种方法，一种为bagging，另一种为boost\n",
    "1. bagging算法里的多个模型都是同个算法，例如都是决策树，但是利用不同的训练样本来制造模型的差异性，训练样本用随机抽取再放回的方式，知道训练集的大小与原来的一样，故会有重复的样本，比如随机森林。\n",
    "2. boost算法的训练需要序列化的训练，但是对数据设置权重，分类错误的样本提升权重，故一开始对所有样本的权重设为1进行训练，针对分类错误的样本提升权重，对的样本降低权重，直到错误率降低至一个阈值，比如adaboost。\n",
    "\n",
    "接下来讲Adaboost具体的算法\n",
    "1. 初始化所有训练数据的权重为1/N。\n",
    "2. 训练分类器，如果某个样本正确分类，则降低权重，错误分类则提升权重，再基于这些权重进行训练，反复迭代。我们有错误率$\\epsilon $以及$\\alpha =\\frac{1}{2}ln(\\frac{1-\\epsilon }{\\epsilon })$，我们有个权重向量V，对于分类正确的样本，乘$\\frac{e^{-\\alpha}}{sum(V)}$，对于分类错误的样本，乘$\\frac{e^{-\\alpha}}{sum(V)}$\n",
    "3. 最后一步则是将所有序列化训练的分类器组成集成学习。但是对分类误差低的分类器增加权重，再进行投票。\n",
    "\n",
    "### 优缺点\n",
    "\n",
    "优点：精准度高、复现容易、不需要调试超参数  \n",
    "缺点：对离群值敏感\n",
    "\n",
    "我们先用最简单的决策树分类器来演示adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'dim': 0, 'ineq': 'lt', 'thresh': 1.3}, matrix([[0.2]]), array([[-1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [ 1.]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loadSimpData():\n",
    "    datMat = np.matrix([[ 1. ,  2.1],\n",
    "        [ 2. ,  1.1],\n",
    "        [ 1.3,  1. ],\n",
    "        [ 1. ,  1. ],\n",
    "        [ 2. ,  1. ]])\n",
    "    classLabels = [1.0, 1.0, -1.0, -1.0, 1.0]\n",
    "    return datMat,classLabels\n",
    "\n",
    "def stumpClassify(dataMatrix,dimen,threshVal,threshIneq):#just classify the data\n",
    "    retArray = np.ones((np.shape(dataMatrix)[0],1))\n",
    "    if threshIneq == 'lt':\n",
    "        retArray[dataMatrix[:,dimen] <= threshVal] = -1.0\n",
    "    else:\n",
    "        retArray[dataMatrix[:,dimen] > threshVal] = -1.0\n",
    "    return retArray\n",
    "    \n",
    "\n",
    "def buildStump(dataArr,classLabels,D):\n",
    "    dataMatrix = np.mat(dataArr); labelMat = np.mat(classLabels).T\n",
    "    m,n = np.shape(dataMatrix)\n",
    "    numSteps = 10.0; bestStump = {}; bestClasEst = np.mat(np.zeros((m,1)))\n",
    "    minError = np.inf #init error sum, to +infinity\n",
    "    for i in range(n):#loop over all dimensions\n",
    "        rangeMin = dataMatrix[:,i].min(); rangeMax = dataMatrix[:,i].max();\n",
    "        stepSize = (rangeMax-rangeMin)/numSteps\n",
    "        for j in range(-1,int(numSteps)+1):#loop over all range in current dimension\n",
    "            for inequal in ['lt', 'gt']: #go over less than and greater than\n",
    "                threshVal = (rangeMin + float(j) * stepSize)\n",
    "                predictedVals = stumpClassify(dataMatrix,i,threshVal,inequal)#call stump classify with i, j, lessThan\n",
    "                errArr = np.mat(np.ones((m,1)))\n",
    "                errArr[predictedVals == labelMat] = 0\n",
    "                weightedError = D.T*errArr  #calc total error multiplied by D\n",
    "                #print \"split: dim %d, thresh %.2f, thresh ineqal: %s, the weighted error is %.3f\" % (i, threshVal, inequal, weightedError)\n",
    "                if weightedError < minError:\n",
    "                    minError = weightedError\n",
    "                    bestClasEst = predictedVals.copy()\n",
    "                    bestStump['dim'] = i\n",
    "                    bestStump['thresh'] = threshVal\n",
    "                    bestStump['ineq'] = inequal\n",
    "    return bestStump,minError,bestClasEst\n",
    "D=np.mat(np.ones((5,1))/5)\n",
    "dataMat, labels = loadSimpData()\n",
    "buildStump(dataMat, labels, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total error:  0.2\n",
      "total error:  0.2\n",
      "total error:  0.0\n"
     ]
    }
   ],
   "source": [
    "def adaBoostTrainDS(dataArr,classLabels,numIt=40):\n",
    "    weakClassArr = []\n",
    "    m = np.shape(dataArr)[0]\n",
    "    D = np.mat(np.ones((m,1))/m)   #init D to all equal\n",
    "    aggClassEst = np.mat(np.zeros((m,1)))\n",
    "    for i in range(numIt):\n",
    "        bestStump,error,classEst = buildStump(dataArr,classLabels,D)#build Stump\n",
    "        #print \"D:\",D.T\n",
    "        alpha = float(0.5*np.log((1.0-error)/max(error,1e-16)))#calc alpha, throw in max(error,eps) to account for error=0\n",
    "        bestStump['alpha'] = alpha  \n",
    "        weakClassArr.append(bestStump)                  #store Stump Params in Array\n",
    "        #print \"classEst: \",classEst.T\n",
    "        expon = np.multiply(-1*alpha*np.mat(classLabels).T,classEst) #exponent for D calc, getting messy\n",
    "        D = np.multiply(D,np.exp(expon))                              #Calc New D for next iteration\n",
    "        D = D/D.sum()\n",
    "        #calc training error of all classifiers, if this is 0 quit for loop early (use break)\n",
    "        aggClassEst += alpha*classEst\n",
    "        #print \"aggClassEst: \",aggClassEst.T\n",
    "        aggErrors = np.multiply(np.sign(aggClassEst) != np.mat(classLabels).T,np.ones((m,1)))\n",
    "        errorRate = aggErrors.sum()/m\n",
    "        print(\"total error: \",errorRate)\n",
    "        if errorRate == 0.0: break\n",
    "    return weakClassArr,aggClassEst\n",
    "classifiers, aggClassEst = adaBoostTrainDS(dataMat, labels, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaClassify(datToClass,classifierArr):\n",
    "    dataMatrix = np.mat(datToClass) # do stuff similar to last aggClassEst in adaBoostTrainDS\n",
    "    m = np.shape(dataMatrix)[0]\n",
    "    aggClassEst = np.mat(np.zeros((m,1)))\n",
    "    for i in range(len(classifierArr)):\n",
    "        classEst = stumpClassify(dataMatrix,int(classifierArr[i]['dim']),\\\n",
    "                                 classifierArr[i]['thresh'],\\\n",
    "                                 classifierArr[i]['ineq']) # call stump classify\n",
    "        aggClassEst += classifierArr[i]['alpha']*classEst\n",
    "#         print(aggClassEst)\n",
    "    return np.sign(aggClassEst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们接下来我们使用之前判断马的生死数据进行示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet(fileName):      #general function to parse tab -delimited floats\n",
    "    numFeat = len(open(fileName).readline().split('\\t')) #get number of fields \n",
    "    dataMat = []; labelMat = []\n",
    "    fr = open(fileName)\n",
    "    for line in fr.readlines():\n",
    "        lineArr =[]\n",
    "        curLine = line.strip().split('\\t')\n",
    "        for i in range(numFeat-1):\n",
    "            lineArr.append(float(curLine[i]))\n",
    "        dataMat.append(lineArr)\n",
    "        labelMat.append(float(curLine[-1]))\n",
    "    return dataMat,labelMat\n",
    "dataMat, labels = loadDataSet('datasets/horseColicTraining2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total error:  0.2842809364548495\n",
      "total error:  0.2842809364548495\n",
      "total error:  0.24749163879598662\n",
      "total error:  0.24749163879598662\n",
      "total error:  0.25418060200668896\n",
      "total error:  0.2408026755852843\n",
      "total error:  0.2408026755852843\n",
      "total error:  0.22073578595317725\n",
      "total error:  0.24749163879598662\n",
      "total error:  0.23076923076923078\n",
      "total error:  0.2408026755852843\n",
      "total error:  0.2140468227424749\n",
      "total error:  0.22742474916387959\n",
      "total error:  0.21739130434782608\n",
      "total error:  0.22073578595317725\n",
      "total error:  0.21739130434782608\n",
      "total error:  0.22408026755852842\n",
      "total error:  0.22408026755852842\n",
      "total error:  0.23076923076923078\n",
      "total error:  0.22408026755852842\n"
     ]
    }
   ],
   "source": [
    "classifiers, aggClassEst=adaBoostTrainDS(dataMat, labels, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testMat,test_labels=loadDataSet('datasets/horseColicTest2.txt')\n",
    "predicts = adaClassify(testKMat,classifiers)\n",
    "errArr=np.mat(np.ones((67,1)))\n",
    "errArr[predicts!=np.mat(test_labels).T].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "错误率来到0.15，之前使用逻辑回归预测马是死是活的错误率是0.34左右"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
